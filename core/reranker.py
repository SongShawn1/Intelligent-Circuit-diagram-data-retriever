#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Reranker æ¨¡å— - ä½¿ç”¨ Cross-Encoder å¯¹æ£€ç´¢ç»“æœè¿›è¡Œç²¾æ’

å·¥ä½œåŸç†ï¼š
1. å‘é‡æœç´¢ï¼ˆBi-Encoderï¼‰æ˜¯ç²—æ’ï¼šç‹¬ç«‹ç¼–ç  Query å’Œ Docï¼Œé€Ÿåº¦å¿«ä½†ç²¾åº¦æœ‰é™
2. Rerankerï¼ˆCross-Encoderï¼‰æ˜¯ç²¾æ’ï¼šåŒæ—¶ç¼–ç  Query-Doc å¯¹ï¼Œç²¾åº¦é«˜ä½†é€Ÿåº¦æ…¢

å…¸å‹æµç¨‹ï¼š
    Query â†’ Vector Search (Top-50) â†’ Reranker ç²¾æ’ â†’ è¿”å› Top-10

æ”¯æŒçš„ Rerankerï¼š
1. BGE Rerankerï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰- éœ€è¦ GPU æˆ–å¤§å†…å­˜
2. API Rerankerï¼ˆæ™ºè°±/Cohereï¼‰- è½»é‡çº§ï¼Œæ¨è
"""

import os
import logging

# ç¦ç”¨ tokenizers å¹¶è¡Œè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from abc import ABC, abstractmethod

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class BaseReranker(ABC):
    """Reranker åŸºç±»"""
    
    @abstractmethod
    def rerank(self, query: str, documents: List[Dict], k: int = 10) -> List[Dict]:
        """
        å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œé‡æ’åº
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¯ dictï¼ŒåŒ…å« content/metadata
            k: è¿”å› top-k ä¸ªç»“æœ
            
        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        raise NotImplementedError


class BgeReranker(BaseReranker):
    """
    BGE Reranker - ä½¿ç”¨æœ¬åœ° Cross-Encoder æ¨¡å‹
    
    æ¨¡å‹é€‰æ‹©ï¼š
    - BAAI/bge-reranker-base: ä¸­ç­‰å¤§å°ï¼Œå¹³è¡¡æ€§èƒ½
    - BAAI/bge-reranker-large: æ›´å¤§æ›´å‡†ç¡®
    - BAAI/bge-reranker-v2-m3: å¤šè¯­è¨€ç‰ˆæœ¬
    """
    
    def __init__(self, model_path: str = 'BAAI/bge-reranker-base'):
        """
        åˆå§‹åŒ– BGE Reranker
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹å
        """
        self.model_path = model_path
        self._model = None
        self._tokenizer = None
        self._device = None
        self._use_fp16 = False  # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
        
    def _ensure_loaded(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self._model is None:
            self._load_model()
            
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        import torch
        from transformers import AutoModelForSequenceClassification, AutoTokenizer
        
        print(f"ğŸ”„ åŠ è½½ Reranker æ¨¡å‹: {self.model_path}")
        
        # é€‰æ‹©è®¾å¤‡ - å¯¹äºå°æ‰¹é‡æ¨ç†ï¼ŒCPU å¯èƒ½æ¯” MPS æ›´å¿«ï¼ˆé¿å…è®¾å¤‡åŒæ­¥å¼€é”€ï¼‰
        # å¼ºåˆ¶ä½¿ç”¨ CPU ä»¥è·å¾—æ›´ç¨³å®šçš„æ€§èƒ½
        self._device = torch.device("cpu")
        self._use_fp16 = False
        print("  â””â”€ ä½¿ç”¨ CPU (å°æ‰¹é‡ä¼˜åŒ–)")
        
        self._tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self._model = AutoModelForSequenceClassification.from_pretrained(
            self.model_path,
            torch_dtype=torch.float32  # CPU ä½¿ç”¨ FP32
        )
        self._model.to(self._device)
        self._model.eval()
        
        print("  â””â”€ âœ… Reranker åŠ è½½å®Œæˆ")
        
    def rerank(self, query: str, documents: List[Dict], k: int = 10) -> List[Dict]:
        """
        ä½¿ç”¨ BGE Reranker å¯¹æ–‡æ¡£è¿›è¡Œç²¾æ’
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            documents: æ–‡æ¡£åˆ—è¡¨
            k: è¿”å›æ•°é‡
            
        Returns:
            é‡æ’åºåçš„ top-k æ–‡æ¡£
        """
        import torch
        
        if not documents:
            return []
            
        self._ensure_loaded()
        
        # æ„å»º Query-Document å¯¹
        # ä½¿ç”¨ filename + ç›¸å…³å­—æ®µä½œä¸ºæ–‡æ¡£å†…å®¹
        pairs = []
        for doc in documents:
            meta = doc.get('metadata', {})
            # æ„å»ºæ–‡æ¡£è¡¨ç¤º
            doc_text = self._build_doc_text(meta)
            pairs.append((query, doc_text))
        
        # æ‰¹é‡è®¡ç®—åˆ†æ•°ï¼ˆä¼˜åŒ–ï¼šå‡å°‘ max_lengthï¼Œå¯ç”¨ç¼–è¯‘ä¼˜åŒ–ï¼‰
        with torch.no_grad():
            inputs = self._tokenizer(
                pairs, 
                padding=True, 
                truncation=True, 
                return_tensors='pt',
                max_length=128  # ä»512é™åˆ°128ï¼Œæ–‡æ¡£åé€šå¸¸å¾ˆçŸ­
            )
            inputs = {key: val.to(self._device) for key, val in inputs.items()}
            
            # æ¨ç†
            outputs = self._model(**inputs, return_dict=True)
            scores = outputs.logits.view(-1).float().cpu().numpy()
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_indices = np.argsort(scores)[::-1][:k]
        
        # è¿”å›é‡æ’åºåçš„æ–‡æ¡£ï¼Œé™„å¸¦ rerank_score
        reranked = []
        for idx in sorted_indices:
            doc = documents[idx].copy()
            doc['rerank_score'] = float(scores[idx])
            reranked.append(doc)
            
        return reranked
    
    def _build_doc_text(self, metadata: Dict) -> str:
        """ä» metadata æ„å»ºæ–‡æ¡£æ–‡æœ¬è¡¨ç¤º"""
        parts = []
        
        # æŒ‰é‡è¦æ€§æ’åˆ—å­—æ®µ
        if metadata.get('filename'):
            parts.append(metadata['filename'])
        if metadata.get('brand'):
            parts.append(f"å“ç‰Œ:{metadata['brand']}")
        if metadata.get('series'):
            parts.append(f"ç³»åˆ—:{metadata['series']}")
        if metadata.get('doc_type'):
            parts.append(f"ç±»å‹:{metadata['doc_type']}")
        if metadata.get('diagram_subtype'):
            parts.append(f"å­ç±»å‹:{metadata['diagram_subtype']}")
            
        return ' '.join(parts)


class ZhipuReranker(BaseReranker):
    """
    æ™ºè°± Reranker - ä½¿ç”¨æ™ºè°± AI API
    
    è½»é‡çº§é€‰æ‹©ï¼Œæ— éœ€æœ¬åœ°æ¨¡å‹
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºè°± Reranker
        
        Args:
            api_key: æ™ºè°± API Keyï¼Œä¸ä¼ åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.api_key = api_key or os.getenv("ZHIPUAI_API_KEY")
        self._client = None
        
    def _ensure_client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯"""
        if self._client is None:
            from zhipuai import ZhipuAI
            self._client = ZhipuAI(api_key=self.api_key)
            
    def rerank(self, query: str, documents: List[Dict], k: int = 10) -> List[Dict]:
        """
        ä½¿ç”¨æ™ºè°± API è¿›è¡Œé‡æ’åº
        
        æ³¨æ„ï¼šæ™ºè°±ç›®å‰å¯èƒ½ä¸æä¾› rerank APIï¼Œæ­¤å¤„ä¸ºé¢„ç•™æ¥å£
        å®é™…å¯ç”¨æ€§éœ€è¦ç¡®è®¤
        """
        # æ™ºè°±ç›®å‰å¯èƒ½æ²¡æœ‰ç›´æ¥çš„ rerank API
        # å¯ä»¥ä½¿ç”¨ embedding ç›¸ä¼¼åº¦ä½œä¸º fallback
        # æˆ–è€…ç­‰å¾…æ™ºè°±å¼€æ”¾ rerank æ¥å£
        
        # Fallback: ä¸åšé‡æ’åºï¼Œç›´æ¥è¿”å›
        print("âš ï¸ æ™ºè°± Reranker æš‚æœªå®ç°ï¼Œè¿”å›åŸå§‹ç»“æœ")
        return documents[:k]


class SimpleReranker(BaseReranker):
    """
    ç®€å• Reranker - åŸºäºå…³é”®è¯åŒ¹é…çš„è½»é‡çº§é‡æ’åº
    
    é€‚ç”¨äºï¼š
    - ä¸æƒ³åŠ è½½å¤§æ¨¡å‹
    - å¿«é€ŸéªŒè¯
    - ä½œä¸º fallback
    """
    
    def __init__(self):
        pass
        
    def rerank(self, query: str, documents: List[Dict], k: int = 10) -> List[Dict]:
        """
        åŸºäºå…³é”®è¯åŒ¹é…çš„ç®€å•é‡æ’åº
        
        ç­–ç•¥ï¼š
        1. å®Œå…¨åŒ¹é… query ä¸­çš„è¯ â†’ é«˜åˆ†
        2. è¿ç»­å­ä¸²åŒ¹é… â†’ åŠ åˆ†
        3. ä¿æŒåŸæœ‰ç›¸ä¼¼åº¦ä½œä¸º baseline
        """
        if not documents:
            return []
        
        # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆ2-4å­—çš„ç‰‡æ®µï¼‰
        query_terms = self._extract_terms(query)
        
        scored_docs = []
        for doc in documents:
            meta = doc.get('metadata', {})
            filename = meta.get('filename', '')
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            match_score = self._calc_match_score(query_terms, filename, query)
            
            # åŸå§‹ç›¸ä¼¼åº¦
            original_score = doc.get('similarity', doc.get('score', 0))
            
            # ç»¼åˆåˆ†æ•°: 60% åŸå§‹ç›¸ä¼¼åº¦ + 40% å…³é”®è¯åŒ¹é…
            combined_score = 0.6 * original_score + 0.4 * match_score
            
            doc_copy = doc.copy()
            doc_copy['rerank_score'] = combined_score
            scored_docs.append((combined_score, doc_copy))
        
        # æ’åº
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        return [doc for _, doc in scored_docs[:k]]
    
    def _extract_terms(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆ2-4å­—ç‰‡æ®µï¼‰"""
        terms = []
        # æŒ‰ä¸­æ–‡åˆ†è¯ä¹ æƒ¯ï¼Œæå–è¿ç»­çš„2-4å­—ç‰‡æ®µ
        for length in [4, 3, 2]:
            for i in range(len(text) - length + 1):
                term = text[i:i+length]
                if term not in terms:
                    terms.append(term)
        return terms
    
    def _calc_match_score(self, terms: List[str], filename: str, query: str) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        if not filename:
            return 0.0
        
        score = 0.0
        
        # 1. å®Œæ•´æŸ¥è¯¢åœ¨æ–‡ä»¶åä¸­çš„åŒ¹é…
        if query in filename:
            score += 0.5
        
        # 2. å…³é”®è¯åŒ¹é…
        matched_terms = [t for t in terms if t in filename]
        if terms:
            term_ratio = len(matched_terms) / len(terms)
            score += 0.3 * term_ratio
        
        # 3. å­—ç¬¦çº§åˆ«é‡å 
        query_chars = set(query)
        filename_chars = set(filename)
        if query_chars:
            char_overlap = len(query_chars & filename_chars) / len(query_chars)
            score += 0.2 * char_overlap
        
        return min(score, 1.0)  # å½’ä¸€åŒ–åˆ° [0, 1]


def create_reranker(
    reranker_type: str = 'simple',
    model_path: str = 'BAAI/bge-reranker-base'
) -> BaseReranker:
    """
    åˆ›å»º Reranker å®ä¾‹
    
    Args:
        reranker_type: ç±»å‹ - 'bge', 'zhipu', 'simple'
        model_path: BGE æ¨¡å‹è·¯å¾„
        
    Returns:
        Reranker å®ä¾‹
    """
    if reranker_type == 'bge':
        return BgeReranker(model_path)
    elif reranker_type == 'zhipu':
        return ZhipuReranker()
    elif reranker_type == 'simple':
        return SimpleReranker()
    else:
        raise ValueError(f"Unknown reranker type: {reranker_type}")


class BM25Prefilter:
    """
    BM25 ç²—ç­›å™¨ - åœ¨ Reranker å‰å¿«é€Ÿç­›é€‰å€™é€‰æ–‡æ¡£
    
    å½“æ–‡æ¡£æ•°é‡å¤§æ—¶ï¼ˆå¦‚ > 50ï¼‰ï¼Œå…ˆç”¨ BM25 å¿«é€Ÿç­›é€‰å‡ºå€™é€‰ï¼Œ
    å†ç”¨ Cross-Encoder ç²¾æ’ï¼Œå¯å¤§å¹…æå‡æ€§èƒ½ã€‚
    
    å…¸å‹æµç¨‹ï¼š
        100+ æ–‡æ¡£ â†’ BM25 ç²—ç­› (Top-30) â†’ Reranker ç²¾æ’ (Top-5)
    """
    
    def __init__(self, k1: float = 1.5, b: float = 0.75):
        """
        åˆå§‹åŒ– BM25 å‚æ•°
        
        Args:
            k1: è¯é¢‘é¥±å’Œå‚æ•° (1.2-2.0)
            b: æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–å‚æ•° (0-1)
        """
        self.k1 = k1
        self.b = b
        self._tokenizer = None
    
    def _get_tokenizer(self):
        """è·å–åˆ†è¯å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._tokenizer is None:
            try:
                import jieba
                self._tokenizer = jieba.lcut
                logger.debug("BM25 ä½¿ç”¨ jieba åˆ†è¯")
            except ImportError:
                # Fallback: å­—ç¬¦çº§åˆ†è¯
                self._tokenizer = lambda text: list(text)
                logger.debug("BM25 ä½¿ç”¨å­—ç¬¦åˆ†è¯ (jieba æœªå®‰è£…)")
        return self._tokenizer
    
    def prefilter(
        self, 
        query: str, 
        documents: List[Dict], 
        top_n: int = 30,
        min_score: float = 0.0
    ) -> List[Dict]:
        """
        ä½¿ç”¨ BM25 å¯¹æ–‡æ¡£è¿›è¡Œç²—ç­›
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            documents: æ–‡æ¡£åˆ—è¡¨
            top_n: è¿”å›å‰ N ä¸ªå€™é€‰
            min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
            
        Returns:
            ç²—ç­›åçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¸¦ bm25_scoreï¼‰
        """
        if not documents:
            return []
        
        if len(documents) <= top_n:
            # æ–‡æ¡£æ•°é‡å°‘äºé˜ˆå€¼ï¼Œä¸éœ€è¦ç²—ç­›
            return documents
        
        tokenize = self._get_tokenizer()
        
        # åˆ†è¯
        query_terms = tokenize(query)
        doc_terms_list = []
        for doc in documents:
            meta = doc.get('metadata', {})
            text = self._build_doc_text(meta)
            doc_terms_list.append(tokenize(text))
        
        # è®¡ç®— IDF
        idf = self._compute_idf(query_terms, doc_terms_list)
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        avg_dl = sum(len(terms) for terms in doc_terms_list) / len(doc_terms_list)
        
        # è®¡ç®— BM25 åˆ†æ•°
        scored_docs = []
        for i, (doc, doc_terms) in enumerate(zip(documents, doc_terms_list)):
            score = self._compute_bm25(query_terms, doc_terms, idf, avg_dl)
            if score >= min_score:
                doc_copy = doc.copy()
                doc_copy['bm25_score'] = score
                scored_docs.append((score, doc_copy))
        
        # æ’åº
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        return [doc for _, doc in scored_docs[:top_n]]
    
    def _build_doc_text(self, metadata: Dict) -> str:
        """ä» metadata æ„å»ºæ–‡æ¡£æ–‡æœ¬"""
        parts = []
        for key in ['filename', 'brand', 'series', 'doc_type', 'diagram_subtype', 'path']:
            if metadata.get(key):
                parts.append(str(metadata[key]))
        return ' '.join(parts)
    
    def _compute_idf(self, query_terms: List[str], doc_terms_list: List[List[str]]) -> Dict[str, float]:
        """è®¡ç®— IDF å€¼"""
        import math
        
        N = len(doc_terms_list)
        idf = {}
        
        for term in set(query_terms):
            # åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°
            df = sum(1 for doc_terms in doc_terms_list if term in doc_terms)
            # IDF = log((N - df + 0.5) / (df + 0.5) + 1)
            idf[term] = math.log((N - df + 0.5) / (df + 0.5) + 1)
        
        return idf
    
    def _compute_bm25(
        self, 
        query_terms: List[str], 
        doc_terms: List[str], 
        idf: Dict[str, float],
        avg_dl: float
    ) -> float:
        """è®¡ç®—å•ä¸ªæ–‡æ¡£çš„ BM25 åˆ†æ•°"""
        score = 0.0
        dl = len(doc_terms)
        
        # è®¡ç®—è¯é¢‘
        tf = {}
        for term in doc_terms:
            tf[term] = tf.get(term, 0) + 1
        
        for term in query_terms:
            if term in tf:
                # BM25 å…¬å¼
                term_tf = tf[term]
                numerator = idf.get(term, 0) * term_tf * (self.k1 + 1)
                denominator = term_tf + self.k1 * (1 - self.b + self.b * dl / avg_dl)
                score += numerator / denominator
        
        return score


# å…¨å±€ BM25 å®ä¾‹
_bm25_prefilter: Optional[BM25Prefilter] = None


def get_bm25_prefilter() -> BM25Prefilter:
    """è·å–å…¨å±€ BM25 ç²—ç­›å™¨å®ä¾‹"""
    global _bm25_prefilter
    if _bm25_prefilter is None:
        _bm25_prefilter = BM25Prefilter()
    return _bm25_prefilter
